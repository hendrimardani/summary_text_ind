# -*- coding: utf-8 -*-
"""sumary_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QydF7G32uewrByF6qAIuuh6mL1ZZ3MF9
"""

!pip install transformers==4.20.0
!pip install keras_nlp==0.3.0
!pip install datasets
!pip install huggingface-hub
!pip install nltk
!pip install rouge-score

import logging
import pandas as pd
import nltk
import tqdm
import json
import datasets
import numpy as np
import tensorflow as tf

from sklearn.model_selection import train_test_split
from datasets import Dataset
from itertools import chain
from tensorflow import keras

# The percentage of the dataset you want to split as train and test
TRAIN_TEST_SPLIT = 0.1

MAX_INPUT_LENGTH = 1024  # Maximum length of the input to the model
MIN_TARGET_LENGTH = 5  # Minimum length of the output by the model
MAX_TARGET_LENGTH = 128  # Maximum length of the output by the model
BATCH_SIZE = 8  # Batch-size for training our model
LEARNING_RATE = 1e-6  # Learning-rate for training our model
# LEARNING_RATE = 2e-5  # Learning-rate for training our model
MAX_EPOCHS = 1  # Maximum number of epochs we will train the model for

# This notebook is built on the t5-small checkpoint from the Hugging Face Model Hub
MODEL_CHECKPOINT = "t5-small"

data = []

with open("/kaggle/input/indosum/indosum/train.01.jsonl", 'r') as f:
    json_list = list(f)
    for json_str in json_list:
        d = json.loads(json_str)
        data.append(d)

len(data)
# Output : 14262

data_2 = []
for x in range(1, 6):
    with open(f"/kaggle/input/indosum/indosum/train.0{str(x)}.jsonl", 'r') as f:
        json_list_2 = list(f)
        for json_str_2 in json_list_2:
            a = json.loads(json_str_2)
            data_2.append(a)

len(data_2)

data[0].keys(), len(data)

data_category = []

for x in range(0, 5):
    data_category.append(data_2[x]["category"])
data_category

def paragraphs_to_text(raw_paragraph_list):
    new_paragraph_list = []
    for i, paragraph in enumerate(raw_paragraph_list):
        paragraph_list = []
        for sentence in paragraph:
            sentence = ' '.join(sentence)
            paragraph_list.append(sentence)

        new_paragraph = ' '.join(paragraph_list)
        new_paragraph_list.append(new_paragraph)

    paragraph_str = ' '.join(new_paragraph_list)
    return paragraph_str

paragraphs_to_text(data[0]['paragraphs'])

paragraphs_list = []

# Menggunakan data_2
for x in range(0,len(data)):
    paragraphs_list.append(paragraphs_to_text(data[x]["paragraphs"]))
paragraphs_list[:5]

def summary_to_text(summary):
    new_summary = []
    for x in summary:
        new_summary.append(' '.join(x))
    hasil = ' '.join(new_summary)
    return hasil

summary_to_text(data[0]["summary"])

# summary_list = []
# summary = [summary_list.append(summary_to_text(data[x]["summary"])) for x in range(0, len(data))]
# summary_list[:5]

summary_list = []

for x in range(0, len(data)):
    summary_list.append(summary_to_text(data[x]["summary"]))
summary_list[:5]

data = pd.DataFrame({
    "text":paragraphs_list,
    "summary":summary_list
})
data

train_list, test_list = train_test_split(data, test_size=0.1)
print(f"Jumlah train {len(train_list)}", f"Jumlah test {len(test_list)}")

train = dict(train_list)
test = dict(test_list)

# Ubah kedalam tipe DatasetDict()
train = Dataset.from_dict(train)
test = Dataset.from_dict(test)
data = datasets.DatasetDict({"train":train, "test":test})
data

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)

if MODEL_CHECKPOINT in ["t5-small", "t5-base", "t5-large", "t5-3b", "t5-11b"]:
    prefix = "summarize: "
else:
    prefix = ""

def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["text"]]
    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)

    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples["summary"], max_length=MAX_TARGET_LENGTH, truncation=True
        )

    model_inputs["labels"] = labels["input_ids"]

    return model_inputs

tokenized_datasets = data.map(preprocess_function, batched=True)
tokenized_datasets

from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq

model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)

from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
data_collator

train_dataset = tokenized_datasets["train"].to_tf_dataset(
    batch_size=BATCH_SIZE,
    columns=["input_ids", "attention_mask", "labels"],
    shuffle=True,
    collate_fn=data_collator,
)
test_dataset = tokenized_datasets["test"].to_tf_dataset(
    batch_size=BATCH_SIZE,
    columns=["input_ids", "attention_mask", "labels"],
    shuffle=False,
    collate_fn=data_collator,
)
generation_dataset = (
    tokenized_datasets["test"]
    .shuffle()
    .select(list(range(124)))
    .to_tf_dataset(
        batch_size=BATCH_SIZE,
        columns=["input_ids", "attention_mask", "labels"],
        shuffle=False,
        collate_fn=data_collator,
    )
)

optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)
model.compile(optimizer=optimizer)

import keras_nlp

rouge_l = keras_nlp.metrics.RougeL()


def metric_fn(eval_predictions):
    predictions, labels = eval_predictions
    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    for label in labels:
        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    result = rouge_l(decoded_labels, decoded_predictions)
    # We will print only the F1 score, you can use other aggregation metrics as well
    result = {"RougeL": result["f1_score"]}

    return result

from transformers.keras_callbacks import KerasMetricCallback

metric_callback = KerasMetricCallback(
    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True
)

callbacks = [metric_callback]

# For now we will use our test set as our validation_data
model.fit(
    train_dataset, validation_data=test_dataset, epochs=MAX_EPOCHS, callbacks=callbacks
)

kalimat_prediksi = "Matahari bersinar terik di Lampung. Sinarnya terhalang rimbunnya pepohonan, sehingga hanya menyisakan berkas tipis. Burung-burung berkicau seolah sedang menyanyikan lagu untuk alam. Bunyi riak jernih sungai beradu dengan batu kali berpadu dengan sahutan dari beberapa penghuni hutan yang lainnya. Ya, inilah tempat tinggal Bora, si anak gajah Lampung yang sekarang tengah asyik bermain bersama teman-temannya di sebuah sungai."

kalimat_prediksi

from transformers import pipeline

summarizer = pipeline("summarization", model=model, tokenizer=tokenizer, framework="tf")

summarizer(
    kalimat_prediksi,
    min_length=MIN_TARGET_LENGTH,
    max_length=MAX_TARGET_LENGTH,
)

data["test"][10]["text"]

# model.push_to_hub("transformers-qa", organization="keras-io")
# tokenizer.push_to_hub("transformers-qa", organization="keras-io")

# from transformers import TFAutoModelForSeq2SeqLM

# model = TFAutoModelForSeq2SeqLM.from_pretrained("your-username/my-awesome-model"

# model.save_weights("model_summary_ind.h5")

model = model.load_weights('model_summary_ind.h5')

from transformers import pipeline

summarizer = pipeline("summarization", model=model, tokenizer=tokenizer, framework="tf")

summarizer(
    data["test"][0]["text"],
    min_length=MIN_TARGET_LENGTH,
    max_length=MAX_TARGET_LENGTH,
)